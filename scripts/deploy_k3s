#!/usr/bin/env bash
#vim: set ft=sh:
TOPLEVEL=$(git rev-parse --show-toplevel)
DEFAULT_REPROVISION=false
DEFAULT_VAGRANT_CWD="$TOPLEVEL/infra/k3s"
PATH_TO_TEMP_KUBECONFIG="/tmp/k3s-kubeconfig"
export VAGRANT_CWD="${VAGRANT_CWD:-$DEFAULT_VAGRANT_CWD}"
export REPROVISION="${REPROVISION:-$DEFAULT_REPROVISION}"

usage() {
 cat <<-USAGE
$(basename $0)
Deploys a k3s cluster.

ENVIRONMENT VARIABLES

  VAGRANT_CWD           Sets the directory in which the Vagrantfile for
                        our k3s cluster lives. [default: $DEFAULT_VAGRANT_CWD]

  REPROVISION           Re-provisions existing machines. Useful for testing
                        changed Ansible playbooks. [default: $DEFAULT_REPROVISION]

NOTES

  - To destroy a cluster, use scripts/destroy_k3s.
  - If you want to interact with these nodes (via SSH, SCP, or similar),
    use \`k3s_vagrant\` instead of Vagrant. You can get this alias by running
    \`source shortcuts\`.
USAGE
}

ensure_vagrant_is_present_and_working() {
  ! test -z "$(vagrant --version)"
}
ensure_vbox_is_present() {
  ! test -z "$(vboxmanage --version)"
}
generate_ssh_keypair() {
  ssh_key=$VAGRANT_CWD/ssh_key
  if ! test -f "$ssh_key"
  then
    ssh-keygen -t rsa -f "$ssh_key" -q -N "" && chmod 600 "$ssh_key"
  fi
}
provision_nodes() {
  if [ "$(echo $REPROVISION | tr '[:upper:]' '[:lower:]')" != "false" ]
  then
    vagrant up --provision
  else
    vagrant up
  fi
}
configure_nodes() {
  docker-compose run --volume="$VAGRANT_CWD:/work" \
    --rm \
    ansible-local \
      ansible-playbook \
      --extra-vars "k3s_token=$(shasum $VAGRANT_CWD/ssh_key | cut -f1 -d ' ' | head -c 8)" \
      --inventory="/work/inventory"  \
      --private-key="/work/ssh_key" \
      --user=vagrant \
      "/work/site.yml"
}

fetch_kubeconfig_for_master() {
  set -x
  command="vagrant ssh k3s-node-0 -c 'sudo cat /etc/rancher/k3s/k3s.yaml' > $PATH_TO_TEMP_KUBECONFIG"
  if ! eval "$command"
  then
    >&2 echo "ERROR: Failed to write kubeconfig locally. Try running it manually: \
$command"
    exit 1
  fi
  set +x
}

check_for_kubectl_and_helm() {
  if ! &>/dev/null which kubectl || ! &>/dev/null which helm
  then
    >&2 echo "WARN: kubectl and/or helm not found"'!'
  fi
}

provide_kubectl_instructions() {
  echo "INFO: Use 'k3skubectl' and 'k3shelm' to access your new k3s cluster."
}

if [ "$1" == '--help' ] || [ "$1" == '-h' ]
then
  usage
  exit 0
fi

if ! ensure_vagrant_is_present_and_working || ! ensure_vbox_is_present
then
  >&2 echo "ERROR: Virtualbox and Vagrant must be installed and working to continue."
  exit 1
fi

generate_ssh_keypair && provision_nodes && configure_nodes &&
  fetch_kubeconfig_for_master && check_for_kubectl_and_helm &&
  provide_kubectl_instructions
